paper_id,title,authors,modalities,task,best_reported_metric,metric_name,dataset_link,notes,difference_to_ProjectA
paper1,"MMIST-ccRCC: A Real World Medical Dataset for the Development of Multi-Modal Systems","Tiago Mota; M. Rita Verdelho; Diogo J. Araújo; Alceu Bissoto; Carlos Santiago; Catarina Barata","CT; MRI; WSI (histopathology); Genomics (VHL,PBRM1,TTN); Clinical","12-month survival prediction","84.7","Balanced Accuracy (BAcc)","https://multi-modalist.github.io/datasets/ccRCC","Curated 618-patient dataset; early/late fusion baselines; MIL for selecting scans; latent reconstruction for missing modalities; best reported: early-fusion mean + reconstruction = BAcc 84.7%","MMIST-ccRCC uses simple early/late fusion and latent reconstruction; Project A differs by explicitly modeling cross-modal interactions with a patient-level Graph Attention Network (nodes = modalities) to capture conditional modality dependencies (e.g., morphology × mutation), and will provide per-patient attention interpretability and Cox survival modelling."
paper2,"Paper 2 (fetched PDF) - modular multimodal fusion","(authors as on PDF)","modal-specific encoders; imaging; clinical; molecular (varies)","prognosis / outcome prediction","(metric reported in PDF)","(metric)","(dataset link if present)","Modular pipeline combining modality-specific encoders, attention-weighted fusion, and survival/regression heads. Ablations show fusion > single-modality; investigates missing-modality robustness via modality dropout.","Focuses on encoder+attention fusion and robustness to missing modalities; differs from Project A which builds an explicit modality-node patient graph (GAT) to learn pairwise cross-modal attention and combine with MIL and Cox models for survival; Project A emphasizes explicit cross-modal interactions conditioned on patient context, not just weighted fusion."
paper3,"Paper 3 (fetched PDF) - WSI MIL & contrastive pretraining","(authors as on PDF)","WSI (histopathology)","subtyping / prognostic prediction from WSI","(metric reported in PDF)","(metric)","(dataset link if present)","Patch-based WSI pipeline: tissue segmentation, patch extraction, ResNet backbone, MIL aggregation with attention, contrastive pretraining; produces interpretable attention heatmaps aligning with pathologist ROIs.","Focuses on WSI-only modeling with MIL and advanced pretraining; Project A uses MIL for WSI tile encoding too but integrates WSI embeddings as one node in a patient-level modality graph together with genomics and clinical nodes — enabling cross-modal attention (e.g., which genomic features modulate which tiles) rather than WSI-only explanations."
paper4,"Paper 4 (fetched PDF) - latent imputation for missing modalities","(authors as on PDF)","multimodal imaging + clinical/genomic","handle missing modalities / prognosis","(metric reported in PDF)","(metric)","(dataset link if present)","Generative latent-feature reconstruction for missing modalities (encoder-decoder + cross-modal losses); shows latent-generation is more robust than zero- or simple imputations, especially for frequent modalities.","Focuses on generative imputation of missing modality latents; Project A instead addresses cross-modal dependency modeling via GAT on available modalities and uses attention interpretability — can be combined with latent imputation, but core novelty is explicit graph-based interaction modeling for survival."
paper5,"Paper 5 (fetched PDF) - multimodal transformer with masked reconstruction","(authors as on PDF)","imaging tokens; clinical; omics","patient-level outcome prediction","(metric reported in PDF)","(metric)","(dataset link if present)","Transformer ingesting variable-length modality tokens with masked-modality reconstruction loss; shows improved calibration and robustness with masked reconstruction regularizer; attention interpreted for modality interactions.","Uses token-level transformer fusion and masked reconstruction; Project A differs by constructing a compact patient graph where modalities are nodes (not many tokens per scan) and applying GAT/GNN to explicitly learn pairwise modality attention for interpretability and survival modeling rather than per-token transformer attention patterns."
paper6,"Paper 6 (fetched PDF) - dataset curation & reproducible baselines","(authors as on PDF)","CT; MRI; WSI; clinical; genomics (varied)","benchmarks / reproducible evaluation for prognosis","(baseline metrics provided)","(metric)","(dataset link if present)","Dataset curation and release of precomputed features, baseline models and evaluation scripts; documents pitfalls (data leakage, time-dependent labels) and best practices for survival analysis.","Infrastructure-focused: provides reproducible baselines and features (useful for Project A). Project A builds on this curation but advances modeling by adding patient-level modality GAT fusion and survival-specific heads (Cox + 12-month classifier) with modality- and feature-level interpretability."
paper7,"AAT-CGF: A Cross-Modal Deep Fusion Framework with Attention Aggregation and Cross Graph Fusion for Multimodal Emotion Recognition","Zhangcheng Yang; Xuebin Zhang; Longting Xu","Audio; Visual; Text","multimodal emotion recognition (CMU-MOSI, IEMOCAP, CASIA)","88.4","Accuracy (7-class)","DOI: 10.1109/IALP68296.2024.11156849","AAT (attention aggregation) + CGF (cross-modal graph fusion) combining modality encoders (LSTM/BERT) with a small graph representing modalities; strong emotion recognition results; ablations show AAT and CGF important.","Also uses graph fusion, but in affective computing on per-frame/utterance data and builds modality-node graphs for cross-modal interaction. Project A differs in domain (oncology), in node design (patient-level modalities including WSI/multiple scans/genomics/clinical), in objective (survival/Cox and 12-month classification) and in integration with MIL WSI encoders and domain-specific genomics features; Project A emphasizes clinical interpretability and survival modeling rather than emotion recognition."
paper8,"CemoBAM: Advancing Multimodal Emotion Recognition through Heterogeneous Graph Networks and Cross-Modal Attention Mechanisms","Nhut Minh Nguyen; Thu Thuy Le; Thanh Trung Nguyen; Duc Tai Phan; Anh Khoa Tran; Duc Ngoc Minh Dang","Audio (Wav2Vec features); Text (BERT embeddings)","multimodal speech emotion recognition (IEMOCAP, ESD)","95.09","Weighted Accuracy (WA %)","https://github.com/nhut-ngnn/CemoBAM","CH-GAT (Top-K heterogeneous graph) + xCBAM (cross-modal transformer + 1D-CBAM); IEMOCAP WA 82.17; ESD WA 95.09; MIN fusion best; Top-K tuning and ablations provided.","Also uses graph attention (CH-GAT) to model cross-modal relations at token/node level in emotion tasks. Project A differs by: (1) clinical domain and survival outcomes, (2) graph nodes correspond to whole modalities per patient (WSI/genomics/clinical), (3) integration with MIL-selected WSI embeddings and survival prediction (Cox + 12-month classifier), and (4) explicit modality- & feature-level interpretability for clinicians."
paper9,"Multimodal information fusion based on event embeddings and spatial-temporal graph convolution networks","Qiuwei Deng; Fei Yin; Yunlong Tian; Wentao Zhang","Audio; Image; Environment/IoT events (event embeddings)","multimodal fusion for smart-home perception / intent recognition","KL 0.08; MSE 0.0234","KL divergence; MSE",,"UHomeMM: event2vec/node2vec + enhanced STGCN with spatial & temporal attention, Transformer fusion encoder, MI + RNN dynamic weight adjustment; results on augmented VGGSound/GTEA/EGTEA (≈280K pairs): KL 0.08, rejection MSE 0.0234; model ~110M params.","Uses spatial-temporal graph convolution for sensor/event fusion in smart-home scenarios. Project A differs by: (1) modeling cross-modal biological relationships in patient data (WSI/genomics/clinical) rather than event sequences, (2) using a GAT-style patient-modality graph for interpretability per patient, and (3) optimizing survival losses (Cox/C-index) rather than KL/MSE for fusion fidelity."
paper10,"Multimodal Learning Data Fusion and Analysis Based on Self-Attention Mechanism","Yi Yue","Text (BERT/Transformer); Image (CNN+Transformer); Audio (CNN+BiLSTM)","Multimodal classification & regression (MM-IMDb, MELD, MOSI)","MM-IMDb ACC 74.30%; MELD ACC 68.20%; MOSI MAE 0.81 / Corr 0.672","ACC / MAE / Corr",,"CMAA (cross-modal adaptive attention) + hierarchical global/local & temporal alignment + GNN-based multi-view fusion; shows gains over TFN/LMF/MulT on listed benchmarks; hyperparameters: hidden=512, LR=0.0005, batch=32.","Leverages self-attention + hierarchical alignment + GNN fusion across modalities in general multimodal tasks. Project A differs by applying a patient-level modality-node GAT specifically designed for RCC prognosis (integrating MIL WSI embeddings, gene mutation encodings, and clinical features), and optimizing survival-specific objectives with per-patient attention interpretability for clinicians."